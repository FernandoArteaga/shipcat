use std::path::Path;
use tokio::fs;

use crate::{
    diff, helm,
    kubeapi::ShipKube,
    kubectl, track,
    webhooks::{self, UpgradeState},
};
use serde_json::json;

use shipcat_definitions::{
    status::{make_date, Condition},
    structs::{Metadata, NotificationMode},
    Config, Manifest, PrimaryWorkload, ReconciliationMode, Region,
};

use super::{ErrorKind, Result, ResultExt};

/// Information from an upgrade
///
/// This information is generated by apply on a best-effort basis.
/// It is provided for webhooks and notifications to make good interfaces.
#[derive(Clone, Debug)]
pub struct UpgradeInfo {
    /// Name of service
    pub name: String,
    /// Metadata for service
    pub metadata: Metadata,
    /// Slack NotificationMode for the service
    pub slackMode: NotificationMode,
    /// Validated version string
    pub version: String,
    /// Validated region requested for installation
    pub region: String,
    /// Validated namespace inferred from region
    pub namespace: String,
    /// Computed diff string (if available)
    pub diff: Option<String>,
}

impl UpgradeInfo {
    /// Export the information needed for book-keeping
    ///
    /// Requires a valid manifest with a pre-determined version
    pub fn new(mf: &Manifest) -> Self {
        UpgradeInfo {
            name: mf.name.clone(),
            version: mf
                .version
                .clone()
                .expect("version must be set before calling UpgradeInfo::new"),
            metadata: mf
                .metadata
                .clone()
                .expect("metadata must exist on every manifest"),
            slackMode: mf.upgradeNotifications.clone().unwrap_or_default(),
            region: mf.region.clone(),
            namespace: mf.namespace.clone(),
            diff: None,
        }
    }
}

/// shipcat apply
///
/// This is the main entrypoint for cli upgrades of a service in a region.
/// It now only supports the shipcat controlled CRD upgrade.
/// Tiller support has been removed.
///
/// The design goals of this is to support:
/// - shipcat apply mysvc
/// - shipcat apply mysvc -t semver
/// In both non-rolling environments (preprod/prod), and rolling envs (complement).
///
/// To allow version-less apply in rolling environment, we have to verify with kube
/// what the current version is.
///
/// The preparatory sanity
///
/// This is entry point should be used by both the cli to apply, and reconcile.
/// It is also entirely responsible for sending webhooks on errors / successes.
/// As such, it's entirely responsible for not propagating random errors here with `?`
/// Every error cases is something that might need to be notified.
pub async fn apply(
    svc: String,
    force: bool,
    region: &Region,
    conf: &Config,
    wait: bool,
    passed_version: Option<String>,
) -> Result<Option<UpgradeInfo>> {
    match region.reconciliationMode {
        ReconciliationMode::CrdOwned => apply_kubectl(&svc, force, region, conf, wait, passed_version).await,
    }
}

/// Reason for an apply being allowed through
///
/// Some of these imply others. We pick the strongest one we can.
#[derive(Debug)]
pub enum UpgradeReason {
    /// New service
    NewService,
    /// New version supplied either in manifest or externally in rolling
    VersionChange,
    /// Configuration change in manifests
    ManifestChange,
    /// Secret updated in vault -> secret manager
    SecretChecksum,
    /// Regional / Chart changes
    TemplateDiff,
    /// Something failed (e.g. diff failed to return) and apply was with --force
    Forced,
}

impl ToString for UpgradeReason {
    fn to_string(&self) -> String {
        format!("{:?}", self)
    }
}

/// First version of apply that does not use tiller
///
/// This writes events to uses the shipcatmanifest crd
#[allow(clippy::cognitive_complexity)] // TODO: refactor this!
async fn apply_kubectl(
    svc: &str,
    force: bool,
    region: &Region,
    conf: &Config,
    wait: bool,
    passed_version: Option<String>,
) -> Result<Option<UpgradeInfo>> {
    if let Err(e) = webhooks::ensure_requirements(&region) {
        warn!("Could not ensure webhook requirements: {}", e);
    }
    let mfbase = shipcat_filebacked::load_manifest(&svc, &conf, &region).await?;

    // A version is set EITHER via `-t SOMEVER` on CLI, or pinned in manifest
    if passed_version.is_some() && mfbase.version.is_some() && mfbase.version != passed_version {
        error!("Overriding a pinned version will be undone at next reconcile");
        bail!(
            "Cannot override version for '{}' because it is pinned in manifests",
            svc
        );
    }
    let explicit_version = mfbase.version.clone().or(passed_version);

    if !mfbase.regions.contains(&region.name) {
        bail!(
            "Cannot deploy '{}' to a region it's not configured for in its manifest",
            svc
        );
    }

    // Interact with the kube api to get the shipcatmanifest crd and its .status
    // This lets us work out:
    // - if the service has been installed before (negates the need for a diff)
    // - if we need to apply a new crd (so we have an atomic change)
    // - if we need to interact with secret-manager TODO: do
    let s = ShipKube::new(&mfbase).await?;

    // Next large batch is working out the reason for the upgrade (if any)
    let mut reason = None;

    // Fetch the minimial shipcatmanifest crd to read version + metadata
    // Safe to bail, pre CRD apply. Next run can retry if kube api getter fails.
    let (actual_version, crd) = match s.get_minimal().await {
        Err(e) => {
            // This usually fails because it's not yet installed
            // serde errors should not happen as using minimal variant!
            debug!("Caught: {}", e); // log it anyway
            match explicit_version {
                None => {
                    warn!("'{}' cannot be installed (no version inferable)", svc);
                    return Err(ErrorKind::MissingRollingVersion(svc.into()).into());
                }
                Some(v) => {
                    reason = Some(UpgradeReason::NewService);
                    (v, None)
                }
            }
        }
        Ok(o) => {
            debug!("existing manifest crd: {}={:?}", o.spec.name, o.status);
            let fallback = o.spec.version.clone();
            match explicit_version {
                None => (fallback, Some(o)),
                Some(v) => {
                    if v != fallback {
                        // A version was supplied, and it differs from active CRD
                        reason = Some(UpgradeReason::VersionChange);
                    }
                    (v, Some(o))
                }
            }
        }
    };
    let can_diff = crd.is_some();
    debug!("using {}={}", svc, actual_version);
    // no shoehorning in illegal versions in the crd!
    region.versioningScheme.verify(&actual_version)?;

    // Complete and apply the CRD
    let mfcrd = mfbase.version(actual_version.clone());
    let crd_changed = s.apply(mfcrd.clone()).await?;
    // Cheap reconcile ends here if !changed && !force
    if crd_changed {
        reason = reason.or(Some(UpgradeReason::ManifestChange));
    }
    if reason.is_none() && !force {
        info!("{} up to date (crd check)", svc);
        return Ok(None);
    }

    // Prepare for an actual upgrade now..
    let mut ui = UpgradeInfo::new(&mfcrd);
    webhooks::apply_event(UpgradeState::Pending, &ui, &region, &conf).await;

    // Fetch all the secrets so we can create a completed manifest
    // TODO: check scp.status.secretChecksum against secret-manager instead
    let mut mf = match mfcrd.complete(&region).await {
        Ok(m) => m,
        Err(e) => {
            // Fire failed events if secrets fail to resolve
            webhooks::apply_event(UpgradeState::Failed, &ui, &region, &conf).await;
            s.update_generate_false("SecretFailure", e.description().to_string())
                .await?;
            return Err(e.into());
        }
    };
    // Should have a UID for ownerReferences now
    mf.uid = if let Some(o) = crd {
        o.metadata.uid
    } else {
        match s.get().await {
            // fallback to the one we just created
            Ok(o) => o.metadata.uid,
            Err(e) => {
                debug!("{:?}", e);
                // Fire failed events if crd could not be fetched after its creation
                webhooks::apply_event(UpgradeState::Failed, &ui, &region, &conf).await;
                s.update_generate_false("CrdFailure", e.description().to_string())
                    .await?;
                return Err(e);
            }
        }
    };

    // Create completed kubernetes yaml (via shipcat values | helm template)
    let tfile = format!("{}.kube.gen.yml", svc);
    let tpth = Path::new(".").join(tfile.clone());
    if let Err(e) = helm::template(&mf, Some(tpth)).await {
        // Errors here are obscure, and should not happen, but pass them up anyway
        webhooks::apply_event(UpgradeState::Failed, &ui, &region, &conf).await;
        s.update_generate_false("ResolveFailure", e.description().to_string())
            .await?;
        return Err(e);
    }

    // Attach diff to UpgradeInfo if diffing is possible
    if can_diff {
        // helm diff only supports diffing if already installed..
        match diff_kubectl(&mf, &tfile).await {
            Ok(Some(kdiff)) => {
                ui.diff = Some(kdiff);
                reason = reason.or(Some(UpgradeReason::TemplateDiff));
            }
            Ok(None) => {
                // If we explicitly received no diff, don't try to upgrade
                // This is a stronger diff than CRD-only if this succeeds; STOP.
                info!("{} up to date (full diff check)", svc);
                webhooks::apply_event(UpgradeState::Cancelled, &ui, &region, &conf).await;
                s.update_generate_true().await?; // every force reconcile makes one generate cond
                return Ok(None);
            }
            // If diffing failed, only run the upgrade if we have to:
            Err(e) => {
                warn!("Unable to diff against {}: {}", svc, e);
                if !force && reason.is_none() {
                    // pass on a diff failure
                    webhooks::apply_event(UpgradeState::Cancelled, &ui, &region, &conf).await;
                    s.update_generate_false("DiffFailure", e.description().to_string())
                        .await?;
                    return Ok(None); // but ultimately ignore this in fast reconciles
                }
                reason = reason.or(Some(UpgradeReason::Forced))
            }
        }
    }

    // We cannot be here without a reason now, although you have to convince yourself.
    let ureason = reason.expect("cannot apply without a reason");
    webhooks::apply_event(UpgradeState::Started, &ui, &region, &conf).await;
    s.update_generate_true().await?; // if this fails, stop, want .status to be correct

    match upgrade_kubectl(&mf, &tfile).await {
        Err(e) => {
            error!("{} from {}", e, ui.name);
            webhooks::apply_event(UpgradeState::Failed, &ui, &region, &conf).await;
            let reason = e.description().to_string();
            s.update_apply_false(ureason.to_string(), "ApplyFailure", reason)
                .await?; // TODO: chain
            return Err(e);
        }
        Ok(_) => {
            let _ = s.update_apply_true(ureason.to_string()).await;
            if !wait {
                info!("successfully applied {} (without waiting)", ui.name);
            } else {
                match track::workload_rollout(&mf, &s).await {
                    Ok(true) => {
                        info!("successfully rolled out {}", &ui.name);
                        webhooks::apply_event(UpgradeState::Completed, &ui, &region, &conf).await;
                        s.update_rollout_true(&actual_version).await?;
                    }
                    Ok(false) => {
                        let time = mf.estimate_wait_time();
                        let reason = format!("timed out waiting {}s for rollout", time);
                        //let _ = kubectl::debug_rollout_status(&mf).await;
                        let _ = track::debug(&mf, &s).await;
                        // TODO: collect these for .status call ^?
                        warn!("failed to roll out {}", &ui.name);
                        webhooks::apply_event(UpgradeState::Failed, &ui, &region, &conf).await;
                        s.update_rollout_false("Timeout", reason).await?; // TODO: chain
                        return Err(ErrorKind::UpgradeTimeout(mf.name.clone(), time).into());
                    }
                    Err(e) => {
                        webhooks::apply_event(UpgradeState::Failed, &ui, &region, &conf).await;
                        s.update_rollout_false("RolloutTrackFailure", e.description().to_string())
                            .await?; // TODO: chain
                        return Err(e);
                    }
                }
            }
        }
    };
    // cleanups in non-error cases
    let _ = fs::remove_file(&tfile).await;
    Ok(Some(ui))
}

/// Shell out to kubectl apply
///
/// Assumes you have written your template file from `helm template`
async fn upgrade_kubectl(mf: &Manifest, tfile: &str) -> Result<()> {
    // upgrade it using the same command
    let applyvec = vec![
        "apply".into(),
        format!("-n={}", mf.namespace),
        "-f".into(),
        tfile.into(),
        "--prune".into(),
        // NB: assumes one deploy per namespace
        format!("-l=app.kubernetes.io/name={}", mf.name),
    ];
    info!("kubectl {}", applyvec.join(" "));
    kubectl::kexec(applyvec)
        .await
        .chain_err(|| ErrorKind::KubectlApplyFailure(mf.name.clone()))?;
    Ok(())
}

/// Minified kubectl diff shell out
///
/// Requires kubernetes 1.13
pub async fn diff_kubectl(mf: &Manifest, tfile: &str) -> Result<Option<String>> {
    let namespace = mf.namespace.clone();
    let pth = Path::new(tfile);
    let (kdiffunobfusc, kdifferr, success) = kubectl::diff(pth.to_path_buf(), &namespace).await?;

    let kubediff = diff::obfuscate_secrets(
        kdiffunobfusc, // move this away quickly..
        mf.get_secrets(),
    );
    debug!("Full diff (obfuscated): \n{}", kubediff);

    if !success {
        let err = kdifferr.trim();
        if err != "exit status 1" {
            warn!("diff {} stderr: {}", mf.name, kdifferr.trim());
            bail!(
                "kubectl diff {} returned: {}",
                mf.name,
                kdifferr.lines().next().unwrap()
            );
        }
    }

    let smalldiff = diff::minify(&kubediff);
    Ok(if !smalldiff.is_empty() {
        debug!("{}", kubediff); // full diff for logs
        println!("{}", smalldiff);
        Some(smalldiff)
    } else {
        None
    })
}

/// Restart the workloads associated with a shipcatmanifest
///
/// Optionally wait for the main resource
pub async fn restart(mf: &Manifest, wait: bool) -> Result<()> {
    for w in &mf.workers {
        let r = Restartable {
            name: w.container.name.clone(),
            namespace: mf.namespace.clone(),
            workload: PrimaryWorkload::Deployment,
        };
        trigger_rollout_restart(r).await?; // fire-and-forget for subresources
    }
    let main = Restartable {
        name: mf.name.clone(),
        namespace: mf.namespace.clone(),
        workload: mf.workload.clone(),
    };
    trigger_rollout_restart(main).await?;
    if !wait {
        info!(
            "successfully triggered a restart of {}/{}",
            mf.workload.to_string(),
            mf.name
        );
        return Ok(());
    }
    let sk = ShipKube::new(&mf).await?;
    // wait for primary if we are waiting
    if track::workload_rollout(&mf, &sk).await? {
        info!("successfully restarted {}/{}", mf.workload.to_string(), &mf.name);
        Ok(())
    } else {
        let time = mf.estimate_wait_time();
        let reason = format!("timed out waiting {}s for rollout to restart", time);
        //let _ = kubectl::debug_rollout_status(&mf).await;
        let _ = track::debug(&mf, &sk).await;
        warn!("failed to roll out {}", &mf.name);
        warn!("{}", reason);
        Err(ErrorKind::UpgradeTimeout(mf.name.clone(), time).into())
    }
}

struct Restartable {
    name: String,
    workload: PrimaryWorkload,
    namespace: String,
}
async fn trigger_rollout_restart(r: Restartable) -> Result<()> {
    let restartvec = vec![
        "rollout".into(),
        format!("-n={}", r.namespace),
        "restart".into(),
        format!("{}/{}", r.workload.to_string(), r.name),
    ];
    info!("kubectl {}", restartvec.join(" "));
    kubectl::kexec(restartvec)
        .await
        .chain_err(|| ErrorKind::KubectlApplyFailure(r.name))
}

/// Uninstall a service
///
/// Not meant to be called if the manifest is still installed in the region
/// shipcat::cluster module is responsible for calling this,
/// when (and only when) a service disappears from disk.
pub async fn delete(svc: &str, reg: &Region, conf: &Config) -> Result<()> {
    let s = ShipKube::new_within(&svc, &reg.namespace).await?;
    match s.get().await {
        // audit all events if it's possible to deserialize current crd
        Ok(mfk) => {
            let info = UpgradeInfo::new(&mfk.spec);
            // We notify before we start, because this is potentially a "panic" type notification.
            webhooks::delete_event(&UpgradeState::Started, &info, &reg, &conf).await;
            match s.delete().await {
                Ok(_) => {
                    // NB: we say completed when the api is "done"
                    // This might still trigger finalizers ATM...
                    webhooks::delete_event(&UpgradeState::Completed, &info, &reg, &conf).await;
                    Ok(())
                }
                Err(e) => {
                    webhooks::delete_event(&UpgradeState::Failed, &info, &reg, &conf).await;
                    Err(e)
                }
            }
        }
        // otherwise, just fire and forget...
        Err(e) => {
            warn!("Unable to notify about service deletion: {}", e);
            // The following Result is more important
            s.delete()
                .await
                .chain_err(|| ErrorKind::KubectlApiFailure("delete".into(), svc.into()))
        }
    }
}

/// Extend kubeapi::ShipKube with patching logic for the .status object
///
/// This should arguably live inside apply.rs or shipcat_definitions
impl ShipKube {
    // ====================================================
    // WARNING : PATCH HELL BELOW
    // ====================================================

    // helper to delete accidental flags
    pub async fn update_generate_true(&self) -> Result<()> {
        debug!("Setting generated true");
        let now = make_date();
        let cond = Condition::ok(&self.applier);
        let data = json!({
            "status": {
                "conditions": {
                    "generated": cond
                },
                "summary": {
                    "lastSuccessfulGenerate": now,
                    "lastAction": "Generate",
                }
            }
        });
        self.patch(&data).await
    }

    // Manual helper fn to blat old status data
    #[allow(dead_code)]
    async fn remove_old_props(&self) -> Result<()> {
        // did you accidentally populate the .status object with garbage?
        let _data = json!({
            "status": {
                "conditions": {
                    "apply": null,
                    "rollout": null,
                },
                "summary": null
            }
        });
        unreachable!("I know what i am doing");
        #[allow(unreachable_code)]
        self.patch(&_data).await
    }

    pub async fn update_generate_false(&self, err: &str, reason: String) -> Result<()> {
        debug!("Setting generated false");
        let cond = Condition::bad(&self.applier, err, reason.clone());
        let data = json!({
            "status": {
                "conditions": {
                    "generated": cond
                },
                "summary": {
                    "lastFailureReason": reason,
                    "lastAction": "Generate",
                }
            }
        });
        self.patch(&data).await
    }

    pub async fn update_apply_true(&self, ureason: String) -> Result<()> {
        debug!("Setting applied true");
        let now = make_date();
        let cond = Condition::ok(&self.applier);
        let data = json!({
            "status": {
                "conditions": {
                    "applied": cond
                },
                "summary": {
                    "lastApply": now,
                    "lastSuccessfulApply": now,
                    "lastApplyReason": ureason,
                    "lastAction": "Apply",
                }
            }
        });
        self.patch(&data).await
    }

    pub async fn update_apply_false(&self, ureason: String, err: &str, reason: String) -> Result<()> {
        debug!("Setting applied false");
        let now = make_date();
        let cond = Condition::bad(&self.applier, err, reason.clone());
        let data = json!({
            "status": {
                "conditions": {
                    "applied": cond
                },
                "summary": {
                    "lastApply": now,
                    "lastFailureReason": reason,
                    "lastApplyReason": ureason,
                    "lastAction": "Apply",
                }
            }
        });
        self.patch(&data).await
    }

    pub async fn update_rollout_false(&self, err: &str, reason: String) -> Result<()> {
        debug!("Setting rolledout false");
        let cond = Condition::bad(&self.applier, err, reason.clone());
        let now = make_date();
        let data = json!({
            "status": {
                "conditions": {
                    "rolledout": cond
                },
                "summary": {
                    "lastRollout": now,
                    "lastFailureReason": reason,
                    "lastAction": "Rollout",
                }
            }
        });
        self.patch(&data).await
    }

    pub async fn update_rollout_true(&self, version: &str) -> Result<()> {
        debug!("Setting rolledout true");
        let now = make_date();
        let cond = Condition::ok(&self.applier);
        let data = json!({
            "status": {
                "conditions": {
                    "rolledout": cond
                },
                "summary": {
                    "lastRollout": now,
                    "lastSuccessfulRollout": now,
                    "lastFailureReason": null,
                    "lastAction": "Rollout",
                    "lastSuccessfulRolloutVersion": version,
                }
            }
        });
        self.patch(&data).await
    }
}
